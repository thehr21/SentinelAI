{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43d4e74-7269-4664-a080-4af61945a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0499edb8-3f98-434a-ad7c-ad1d1558532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost version: 3.0.2\n",
      "üöÄ GPU is available and will be used for training.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: CUDA Check\n",
    "from xgboost import XGBClassifier\n",
    "print(\"‚úÖ XGBoost version:\", xgb.__version__)\n",
    "try:\n",
    "    test_model = XGBClassifier(tree_method='hist', device='cuda')\n",
    "    print(\"üöÄ GPU is available and will be used for training.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå GPU check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f0058b7-7610-4ea8-bcdd-8b72b5cbdce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Processing: datasets\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "‚úÖ Saved chunk 1 with 99984 rows\n",
      "‚úÖ Saved chunk 2 with 99994 rows\n",
      "‚úÖ Saved chunk 3 with 25733 rows\n",
      "üì• Processing: datasets\\Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "‚úÖ Saved chunk 4 with 99935 rows\n",
      "‚úÖ Saved chunk 5 with 99811 rows\n",
      "‚úÖ Saved chunk 6 with 86350 rows\n",
      "üì• Processing: datasets\\Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "‚úÖ Saved chunk 7 with 99930 rows\n",
      "‚úÖ Saved chunk 8 with 90981 rows\n",
      "üì• Processing: datasets\\Monday-WorkingHours.pcap_ISCX.csv\n",
      "‚úÖ Saved chunk 9 with 99906 rows\n",
      "‚úÖ Saved chunk 10 with 99915 rows\n",
      "‚úÖ Saved chunk 11 with 99948 rows\n",
      "‚úÖ Saved chunk 12 with 99912 rows\n",
      "‚úÖ Saved chunk 13 with 99906 rows\n",
      "‚úÖ Saved chunk 14 with 29894 rows\n",
      "üì• Processing: datasets\\Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "‚úÖ Saved chunk 15 with 99964 rows\n",
      "‚úÖ Saved chunk 16 with 99886 rows\n",
      "‚úÖ Saved chunk 17 with 88545 rows\n",
      "üì• Processing: datasets\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "‚úÖ Saved chunk 18 with 99910 rows\n",
      "‚úÖ Saved chunk 19 with 70321 rows\n",
      "üì• Processing: datasets\\Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "‚úÖ Saved chunk 20 with 99929 rows\n",
      "‚úÖ Saved chunk 21 with 99943 rows\n",
      "‚úÖ Saved chunk 22 with 99938 rows\n",
      "‚úÖ Saved chunk 23 with 99947 rows\n",
      "‚úÖ Saved chunk 24 with 45888 rows\n",
      "üì• Processing: datasets\\Wednesday-workingHours.pcap_ISCX.csv\n",
      "‚úÖ Saved chunk 25 with 99869 rows\n",
      "‚úÖ Saved chunk 26 with 99629 rows\n",
      "‚úÖ Saved chunk 27 with 99573 rows\n",
      "‚úÖ Saved chunk 28 with 99836 rows\n",
      "‚úÖ Saved chunk 29 with 99913 rows\n",
      "‚úÖ Saved chunk 30 with 99943 rows\n",
      "‚úÖ Saved chunk 31 with 92643 rows\n",
      "‚úÖ All chunks processed and saved as Multi-Class LibSVM\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Convert CSV logs to Multi-Class LibSVM format\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "folder_path = 'datasets'\n",
    "output_libsvm = 'multiclass_dataset.libsvm'\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = LabelEncoder()\n",
    "first_chunk = True\n",
    "label_list = []\n",
    "\n",
    "chunk_count = 0\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f\"üì• Processing: {file}\")\n",
    "    chunks = pd.read_csv(file, chunksize=100_000)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            if 'Label' not in chunk.columns:\n",
    "                continue\n",
    "\n",
    "            # Drop unnecessary columns\n",
    "            drop_cols = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\n",
    "            chunk.drop(columns=[col for col in drop_cols if col in chunk.columns], inplace=True, errors='ignore')\n",
    "\n",
    "            # Filter numeric + label\n",
    "            chunk = chunk.select_dtypes(include=[np.number]).join(chunk['Label'])\n",
    "\n",
    "            chunk.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            chunk.dropna(inplace=True)\n",
    "\n",
    "            # Encode multi-class labels\n",
    "            labels = encoder.fit_transform(chunk['Label'])\n",
    "            y = pd.Series(labels)\n",
    "            label_list.extend(chunk['Label'].unique())  # collect original labels\n",
    "            X = chunk.drop(columns=['Label'])\n",
    "\n",
    "            # Scale features\n",
    "            X_scaled = scaler.fit_transform(X.astype(np.float32)) if first_chunk else scaler.transform(X.astype(np.float32))\n",
    "\n",
    "            # Dump to file with manual append handling\n",
    "            mode = 'ab' if not first_chunk else 'wb'\n",
    "            with open(output_libsvm, mode) as f:\n",
    "                dump_svmlight_file(X_scaled, y, f, zero_based=True)\n",
    "\n",
    "            first_chunk = False\n",
    "            chunk_count += 1\n",
    "            print(f\"‚úÖ Saved chunk {chunk_count} with {len(chunk)} rows\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunk error: {e}\")\n",
    "\n",
    "print(\"‚úÖ All chunks processed and saved as Multi-Class LibSVM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "882d568d-80be-4586-8cf6-6148c019df54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading full multi-class dataset from LibSVM format...\n",
      "‚úÖ Loaded: (2827876, 78)\n",
      "üî¢ Number of classes detected: 4\n",
      "üß† Training multi-class XGBoost model with GPU...\n",
      "‚úÖ Model training complete.\n",
      "üîç Evaluating model...\n",
      "‚úÖ Accuracy: 0.9962257892495994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00   2271320\n",
      "         1.0       0.98      1.00      0.99    522473\n",
      "         2.0       0.96      0.73      0.83     27635\n",
      "         3.0       0.99      0.93      0.96      6448\n",
      "\n",
      "    accuracy                           1.00   2827876\n",
      "   macro avg       0.98      0.91      0.94   2827876\n",
      "weighted avg       1.00      1.00      1.00   2827876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Train XGBoost Multi-Class Model using full dataset (GPU)\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "# Load dataset\n",
    "print(\"üì¶ Loading full multi-class dataset from LibSVM format...\")\n",
    "X, y = load_svmlight_file(\"multiclass_dataset.libsvm\")\n",
    "print(f\"‚úÖ Loaded: {X.shape}\")\n",
    "\n",
    "# Detect number of classes\n",
    "num_classes = len(set(y))\n",
    "print(f\"üî¢ Number of classes detected: {num_classes}\")\n",
    "\n",
    "# Create DMatrix\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': num_classes,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'tree_method': 'hist',      # GPU tip: use 'hist' and add 'device': 'cuda'\n",
    "    'device': 'cuda',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "# Train model\n",
    "print(\"üß† Training multi-class XGBoost model with GPU...\")\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "print(\"‚úÖ Model training complete.\")\n",
    "\n",
    "# Predict\n",
    "print(\"üîç Evaluating model...\")\n",
    "y_pred = model.predict(dtrain)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "\n",
    "# Report\n",
    "acc = accuracy_score(y, y_pred_labels)\n",
    "print(f\"‚úÖ Accuracy: {acc}\")\n",
    "print(classification_report(y, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26e8e267-2f82-4405-8438-0be53c7d6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to: ai\\models\\xgboost_multiclass.model\n",
      "‚úÖ Label encoder saved to: ai\\models\\label_encoder.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\AppData\\Local\\Temp\\ipykernel_31624\\2932314549.py:13: UserWarning: [08:23:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  model.save_model(model_path)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Save Trained Multi-Class Model & Label Encoder\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs(\"ai/models\", exist_ok=True)\n",
    "\n",
    "# Define paths\n",
    "model_path = os.path.join(\"ai\", \"models\", \"xgboost_multiclass.model\")\n",
    "encoder_path = os.path.join(\"ai\", \"models\", \"label_encoder.pkl\")\n",
    "\n",
    "# Save model and encoder\n",
    "model.save_model(model_path)\n",
    "joblib.dump(encoder, encoder_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "print(f\"‚úÖ Label encoder saved to: {encoder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3fa6f31-25c6-4a82-84db-c6b50d20bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and label encoder loaded.\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00   2271320\n",
      "         1.0       0.98      1.00      0.99    522473\n",
      "         2.0       0.96      0.73      0.83     27635\n",
      "         3.0       0.99      0.93      0.96      6448\n",
      "\n",
      "    accuracy                           1.00   2827876\n",
      "   macro avg       0.98      0.91      0.94   2827876\n",
      "weighted avg       1.00      1.00      1.00   2827876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load trained model and encoder for prediction\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Correct paths\n",
    "model_path = os.path.join(\"ai\", \"models\", \"xgboost_multiclass.model\")\n",
    "encoder_path = os.path.join(\"ai\", \"models\", \"label_encoder.pkl\")\n",
    "dataset_path = \"multiclass_dataset.libsvm\"  \n",
    "\n",
    "# Load model and encoder\n",
    "model = xgb.Booster()\n",
    "model.load_model(model_path)\n",
    "encoder = joblib.load(encoder_path)\n",
    "print(\"‚úÖ Model and label encoder loaded.\")\n",
    "\n",
    "# Load dataset\n",
    "X, y_true = load_svmlight_file(dataset_path)\n",
    "\n",
    "# Predict\n",
    "dtest = xgb.DMatrix(X)\n",
    "y_pred_prob = model.predict(dtest)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada98dd6-d00a-454c-b728-50709c078877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SentinelAI)",
   "language": "python",
   "name": "venv-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
