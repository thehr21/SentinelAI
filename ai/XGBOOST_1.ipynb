{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43d4e74-7269-4664-a080-4af61945a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0499edb8-3f98-434a-ad7c-ad1d1558532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost version: 3.0.2\n",
      "ðŸš€ GPU is available and will be used for training.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: CUDA Check\n",
    "from xgboost import XGBClassifier\n",
    "print(\"âœ… XGBoost version:\", xgb.__version__)\n",
    "try:\n",
    "    test_model = XGBClassifier(tree_method='hist', device='cuda')\n",
    "    print(\"ðŸš€ GPU is available and will be used for training.\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ GPU check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0058b7-7610-4ea8-bcdd-8b72b5cbdce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scanning all labels...\n",
      "âœ… Label encoder fitted with classes: ['BENIGN', 'Bot', 'DDoS', 'DoS GoldenEye', 'DoS Hulk', 'DoS Slowhttptest', 'DoS slowloris', 'FTP-Patator', 'Heartbleed', 'Infiltration', 'PortScan', 'SSH-Patator', 'Web Attack ï¿½ Brute Force', 'Web Attack ï¿½ Sql Injection', 'Web Attack ï¿½ XSS']\n",
      "âœ… Saved chunk 1 with 99984 rows\n",
      "âœ… Saved chunk 2 with 99994 rows\n",
      "âœ… Saved chunk 3 with 25733 rows\n",
      "âœ… Saved chunk 4 with 99935 rows\n",
      "âœ… Saved chunk 5 with 99811 rows\n",
      "âœ… Saved chunk 6 with 86350 rows\n",
      "âœ… Saved chunk 7 with 99930 rows\n",
      "âœ… Saved chunk 8 with 90981 rows\n",
      "âœ… Saved chunk 9 with 99906 rows\n",
      "âœ… Saved chunk 10 with 99915 rows\n",
      "âœ… Saved chunk 11 with 99948 rows\n",
      "âœ… Saved chunk 12 with 99912 rows\n",
      "âœ… Saved chunk 13 with 99906 rows\n",
      "âœ… Saved chunk 14 with 29894 rows\n",
      "âœ… Saved chunk 15 with 99964 rows\n",
      "âœ… Saved chunk 16 with 99886 rows\n",
      "âœ… Saved chunk 17 with 88545 rows\n",
      "âœ… Saved chunk 18 with 99910 rows\n",
      "âœ… Saved chunk 19 with 70321 rows\n",
      "âœ… Saved chunk 20 with 99929 rows\n",
      "âœ… Saved chunk 21 with 99943 rows\n",
      "âœ… Saved chunk 22 with 99938 rows\n",
      "âœ… Saved chunk 23 with 99947 rows\n",
      "âœ… Saved chunk 24 with 45888 rows\n",
      "âœ… Saved chunk 25 with 99869 rows\n",
      "âœ… Saved chunk 26 with 99629 rows\n",
      "âœ… Saved chunk 27 with 99573 rows\n",
      "âœ… Saved chunk 28 with 99836 rows\n",
      "âœ… Saved chunk 29 with 99913 rows\n",
      "âœ… Saved chunk 30 with 99943 rows\n",
      "âœ… Saved chunk 31 with 92643 rows\n",
      "âœ… All chunks processed and saved as Multi-Class LibSVM\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Convert CSV logs to Multi-Class LibSVM format (with global label encoder)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "folder_path = 'datasets'\n",
    "output_libsvm = 'multiclass_dataset.libsvm'\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Phase 1: Collect all labels first\n",
    "print(\"ðŸ” Scanning all labels...\")\n",
    "all_labels = []\n",
    "for file in csv_files:\n",
    "    chunks = pd.read_csv(file, chunksize=100_000)\n",
    "    for chunk in chunks:\n",
    "        chunk.columns = chunk.columns.str.strip()\n",
    "        if 'Label' in chunk.columns:\n",
    "            all_labels.extend(chunk['Label'].dropna().unique())\n",
    "all_labels = pd.Series(all_labels).unique()\n",
    "\n",
    "# Fit encoder on all unique labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(all_labels)\n",
    "print(f\"âœ… Label encoder fitted with classes: {list(encoder.classes_)}\")\n",
    "\n",
    "# Phase 2: Process and save dataset\n",
    "scaler = StandardScaler()\n",
    "first_chunk = True\n",
    "chunk_count = 0\n",
    "\n",
    "for file in csv_files:\n",
    "    chunks = pd.read_csv(file, chunksize=100_000)\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            if 'Label' not in chunk.columns:\n",
    "                continue\n",
    "\n",
    "            # Drop extra columns to avoid join/merge confusion\n",
    "            drop_cols = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\n",
    "            chunk.drop(columns=[col for col in drop_cols if col in chunk.columns], inplace=True, errors='ignore')\n",
    "\n",
    "            # Keep only numeric + label\n",
    "            chunk = chunk.select_dtypes(include=[np.number]).join(chunk['Label'])\n",
    "\n",
    "            chunk.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            chunk.dropna(inplace=True)\n",
    "\n",
    "            # Encode labels using pre-fitted encoder\n",
    "            y = encoder.transform(chunk['Label'])\n",
    "            X = chunk.drop(columns=['Label'])\n",
    "\n",
    "            X_scaled = scaler.fit_transform(X.astype(np.float32)) if first_chunk else scaler.transform(X.astype(np.float32))\n",
    "\n",
    "            # Save as LibSVM\n",
    "            mode = 'wb' if first_chunk else 'ab'\n",
    "            with open(output_libsvm, mode) as f:\n",
    "                dump_svmlight_file(X_scaled, y, f, zero_based=True)\n",
    "\n",
    "            first_chunk = False\n",
    "            chunk_count += 1\n",
    "            print(f\"âœ… Saved chunk {chunk_count} with {len(chunk)} rows\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Chunk error: {e}\")\n",
    "\n",
    "print(\"âœ… All chunks processed and saved as Multi-Class LibSVM\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882d568d-80be-4586-8cf6-6148c019df54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loading full multi-class dataset from LibSVM format...\n",
      "âœ… Loaded: (2827876, 78)\n",
      "ðŸ”¢ Number of classes detected: 15\n",
      "ðŸ§  Training multi-class XGBoost model with GPU...\n",
      "âœ… Model training complete.\n",
      "ðŸ” Evaluating model...\n",
      "âœ… Accuracy: 0.9991806571433826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00   2271320\n",
      "         1.0       0.96      0.79      0.87      1956\n",
      "         2.0       1.00      1.00      1.00    128025\n",
      "         3.0       1.00      1.00      1.00     10293\n",
      "         4.0       1.00      1.00      1.00    230124\n",
      "         5.0       0.99      1.00      0.99      5499\n",
      "         6.0       1.00      1.00      1.00      5796\n",
      "         7.0       1.00      1.00      1.00      7935\n",
      "         8.0       1.00      1.00      1.00        11\n",
      "         9.0       1.00      1.00      1.00        36\n",
      "        10.0       0.99      1.00      1.00    158804\n",
      "        11.0       1.00      1.00      1.00      5897\n",
      "        12.0       0.79      0.95      0.86      1507\n",
      "        13.0       1.00      1.00      1.00        21\n",
      "        14.0       0.79      0.43      0.56       652\n",
      "\n",
      "    accuracy                           1.00   2827876\n",
      "   macro avg       0.97      0.94      0.95   2827876\n",
      "weighted avg       1.00      1.00      1.00   2827876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Train XGBoost Multi-Class Model using full dataset (GPU)\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "# Load dataset\n",
    "print(\"ðŸ“¦ Loading full multi-class dataset from LibSVM format...\")\n",
    "X, y = load_svmlight_file(\"multiclass_dataset.libsvm\")\n",
    "print(f\"âœ… Loaded: {X.shape}\")\n",
    "\n",
    "# Detect number of classes\n",
    "num_classes = len(set(y))\n",
    "print(f\"ðŸ”¢ Number of classes detected: {num_classes}\")\n",
    "\n",
    "# Convert to DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "# Define training parameters\n",
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': num_classes,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'tree_method': 'hist',       # Tip: For GPU support, use 'hist' + 'device': 'cuda'\n",
    "    'device': 'cuda',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "print(\"ðŸ§  Training multi-class XGBoost model with GPU...\")\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "print(\"âœ… Model training complete.\")\n",
    "\n",
    "# Predict and evaluate\n",
    "print(\"ðŸ” Evaluating model...\")\n",
    "y_pred = model.predict(dtrain)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "\n",
    "# Evaluation metrics\n",
    "acc = accuracy_score(y, y_pred_labels)\n",
    "print(f\"âœ… Accuracy: {acc}\")\n",
    "print(classification_report(y, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e8e267-2f82-4405-8438-0be53c7d6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to: ai\\models\\xgboost_multiclass.model\n",
      "âœ… Label encoder saved to: ai\\models\\label_encoder.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\AppData\\Local\\Temp\\ipykernel_1776\\3790923986.py:12: UserWarning: [03:14:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  model.save_model(model_path)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Save Trained Multi-Class Model & Label Encoder\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs(\"ai/models\", exist_ok=True)\n",
    "\n",
    "# Define paths\n",
    "model_path = os.path.join(\"ai\", \"models\", \"xgboost_multiclass.model\")\n",
    "encoder_path = os.path.join(\"ai\", \"models\", \"label_encoder.pkl\")\n",
    "\n",
    "# Save model and encoder\n",
    "model.save_model(model_path)\n",
    "joblib.dump(encoder, encoder_path)\n",
    "\n",
    "print(f\"âœ… Model saved to: {model_path}\")\n",
    "print(f\"âœ… Label encoder saved to: {encoder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fa6f31-25c6-4a82-84db-c6b50d20bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and label encoder loaded.\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00   2271320\n",
      "         1.0       0.96      0.79      0.87      1956\n",
      "         2.0       1.00      1.00      1.00    128025\n",
      "         3.0       1.00      1.00      1.00     10293\n",
      "         4.0       1.00      1.00      1.00    230124\n",
      "         5.0       0.99      1.00      0.99      5499\n",
      "         6.0       1.00      1.00      1.00      5796\n",
      "         7.0       1.00      1.00      1.00      7935\n",
      "         8.0       1.00      1.00      1.00        11\n",
      "         9.0       1.00      1.00      1.00        36\n",
      "        10.0       0.99      1.00      1.00    158804\n",
      "        11.0       1.00      1.00      1.00      5897\n",
      "        12.0       0.79      0.95      0.86      1507\n",
      "        13.0       1.00      1.00      1.00        21\n",
      "        14.0       0.79      0.43      0.56       652\n",
      "\n",
      "    accuracy                           1.00   2827876\n",
      "   macro avg       0.97      0.94      0.95   2827876\n",
      "weighted avg       1.00      1.00      1.00   2827876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load trained model and encoder for prediction\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Correct paths\n",
    "model_path = os.path.join(\"ai\", \"models\", \"xgboost_multiclass.model\")\n",
    "encoder_path = os.path.join(\"ai\", \"models\", \"label_encoder.pkl\")\n",
    "dataset_path = \"multiclass_dataset.libsvm\"  \n",
    "\n",
    "# Load model and encoder\n",
    "model = xgb.Booster()\n",
    "model.load_model(model_path)\n",
    "encoder = joblib.load(encoder_path)\n",
    "print(\"âœ… Model and label encoder loaded.\")\n",
    "\n",
    "# Load dataset\n",
    "X, y_true = load_svmlight_file(dataset_path)\n",
    "\n",
    "# Predict\n",
    "dtest = xgb.DMatrix(X)\n",
    "y_pred_prob = model.predict(dtest)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"ðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada98dd6-d00a-454c-b728-50709c078877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and encoder loaded\n",
      "âœ… Predictions saved to: ai\\predictions_with_flags.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Predict, Score, Flag, and Save Results (Fixed)\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "# Paths\n",
    "model_path = os.path.join(\"ai\", \"models\", \"xgboost_multiclass.model\")\n",
    "encoder_path = os.path.join(\"ai\", \"models\", \"label_encoder.pkl\")\n",
    "dataset_path = \"multiclass_dataset.libsvm\"\n",
    "\n",
    "# Load model and encoder\n",
    "model = xgb.Booster()\n",
    "model.load_model(model_path)\n",
    "encoder = joblib.load(encoder_path)\n",
    "print(\"âœ… Model and encoder loaded\")\n",
    "\n",
    "# Load dataset\n",
    "X, y_true = load_svmlight_file(dataset_path)\n",
    "dtest = xgb.DMatrix(X)\n",
    "\n",
    "# Predict probabilities\n",
    "y_probs = model.predict(dtest)\n",
    "y_pred = np.argmax(y_probs, axis=1)\n",
    "\n",
    "# Map encoded class numbers to string labels\n",
    "unique_class_indices = np.unique(y_pred)\n",
    "class_labels = encoder.inverse_transform(unique_class_indices)\n",
    "label_mapping = dict(zip(unique_class_indices, class_labels))\n",
    "\n",
    "predicted_labels = [label_mapping[i] for i in y_pred]\n",
    "true_labels = encoder.inverse_transform(y_true.astype(int))\n",
    "\n",
    "# Compute threat scores: confidence * 100 only for non-benign, else 0\n",
    "threat_scores = []\n",
    "for i, pred_class in enumerate(y_pred):\n",
    "    label = label_mapping[pred_class]\n",
    "    score = float(np.max(y_probs[i])) * 100 if label != \"BENIGN\" else 0.0\n",
    "    threat_scores.append(round(score, 2))\n",
    "\n",
    "# Flag if score â‰¥ 75 and not BENIGN\n",
    "is_flagged = [(label != \"BENIGN\") and (score >= 75) for label, score in zip(predicted_labels, threat_scores)]\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame({\n",
    "    'true_label': true_labels,\n",
    "    'predicted_label': predicted_labels,\n",
    "    'threat_score': threat_scores,\n",
    "    'is_flagged': is_flagged\n",
    "})\n",
    "\n",
    "# Add class-wise confidence columns\n",
    "for i, label in label_mapping.items():\n",
    "    df_results[f'conf_{label}'] = (y_probs[:, i] * 100).round(2)\n",
    "\n",
    "# Save results\n",
    "output_path = os.path.join(\"ai\", \"predictions_with_flags.csv\")\n",
    "df_results.to_csv(output_path, index=False)\n",
    "print(f\"âœ… Predictions saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad3cb69-ca4a-4cfd-9219-63792bcba762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SentinelAI)",
   "language": "python",
   "name": "venv-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
